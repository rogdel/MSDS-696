{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 696: Dog Detection\n",
    "\n",
    "## Rogelio B Delgado\n",
    "\n",
    "### Spring II 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the important libraries necessary to perform this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I set the minimum confidence interval to be at .70 or 70%. This means that while the YOLO model is running the bounding box and confidence score will only populate if the model believes the object is a dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_confidence = 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathered from ultralytics.com, dog pose dataset.\n",
    "https://www.ultralytics.com/blog/custom-training-ultralytics-yolo11-for-dog-pose-estimation\n",
    "\n",
    "I have already ran the model and saved in onto my desktop as best.pt. The traning was done on google code lab because of this computing power, compared to my own cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "#model = YOLO(\"yolo11n-pose.pt\")\n",
    "\n",
    "# Train the model\n",
    "#results = model.train(data=\"dog-pose.yaml\", epochs=100, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple test to ensure that the model was uploaded and trained. The iamge was resized to have a visually appearing picture. The confidnece score was 90%.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing image 1\n",
    "results = model('sit_1.jpg')\n",
    "\n",
    "image_sit1 = results[0].plot()\n",
    "\n",
    "resized_image = cv2.resize(image_sit1, (800, 800))\n",
    "\n",
    "resized_image_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(resized_image_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing image 2\n",
    "results1 = model('sit_3.jpg')\n",
    "\n",
    "image_sit_3 = results1[0].plot()\n",
    "\n",
    "resized_image1 = cv2.resize(image_sit_3, (800, 800))\n",
    "\n",
    "resized_image_rgb1 = cv2.cvtColor(resized_image1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(resized_image_rgb1)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video 1: Atlas Running through the snow in the backyard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_1 = r'video_1.mp4'\n",
    "cap = cv2.VideoCapture(video_1)\n",
    "\n",
    "\n",
    "\n",
    "img_widget = widgets.Image(format='jpeg', width=800, height=800)\n",
    "display(img_widget)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame)\n",
    "    annotated_frame = results[0].plot()\n",
    "    \n",
    "    rotated_frame = cv2.rotate(annotated_frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "    resized_frame = cv2.resize(rotated_frame, (600, 600))\n",
    "\n",
    "    ret2, jpeg = cv2.imencode('.jpg', resized_frame)\n",
    "    if ret2:\n",
    "        img_widget.value = jpeg.tobytes()\n",
    "    \n",
    "    time.sleep(0.0001)\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video of a herd of deer near my workplace. This is an example of how four legged animals were identified as dogs. This is due to the deer having matching points to a dog. Legs, tail, head, torso, ect. The YOLO model is not defined to identify dogs but rather poses. The problem is that all of the training images are that of dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead my project shifted to focus on a specific point, outside my porch door. I only have one dog and do not have other dogs in my backyard from familiy or friends. Thus, no need to identify my own dog from others.\n",
    "\n",
    "The model did an accurate job by keeping the bounding box and the confidence score of .70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_2 = r'Atlas_1.mp4'\n",
    "cap = cv2.VideoCapture(video_2)\n",
    "\n",
    "img_widget = widgets.Image(format='jpeg', width=800, height=800)\n",
    "display(img_widget)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame, conf=min_confidence)\n",
    "    annotated_frame = results[0].plot()\n",
    "    \n",
    "    rotated_frame = cv2.rotate(annotated_frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "    resized_frame = cv2.resize(rotated_frame, (600, 600))\n",
    "\n",
    "    ret2, jpeg = cv2.imencode('.jpg', resized_frame)\n",
    "    if ret2:\n",
    "        img_widget.value = jpeg.tobytes()\n",
    "    \n",
    "    time.sleep(0.0001)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_2 = r\"Atlas_1.mp4\"\n",
    "cap = cv2.VideoCapture(video_2)\n",
    "min_confidence = 0.5\n",
    "\n",
    "window_name = \"Atlas Pose\"\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name, 800, 800)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame, conf=min_confidence)[0]\n",
    "    annotated = results.plot()\n",
    "\n",
    "    rotated = cv2.rotate(annotated, cv2.ROTATE_90_CLOCKWISE)\n",
    "    display_frame = cv2.resize(rotated, (600, 600))\n",
    "\n",
    "    cv2.imshow(window_name, display_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new code attempt at normal speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.pt\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "min_confidence = 0.5\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(r\"Atlas_1.mp4\")\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "frame_interval = 1.0 / fps\n",
    "\n",
    "\n",
    "downscale_size  = (480, 270)\n",
    "display_size    = (600, 600)\n",
    "process_every_n = 3\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    start_time = time.time()\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "    frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % process_every_n != 0:\n",
    "        display_frame = cv2.resize(frame, display_size)\n",
    "    else:\n",
    "\n",
    "        small = cv2.resize(frame, downscale_size)\n",
    "\n",
    "\n",
    "        results = model(small, device=device, conf=min_confidence)[0]\n",
    "\n",
    "\n",
    "        for box in results.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "            conf = box.conf[0].item()\n",
    "\n",
    "            cv2.rectangle(small, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            label = f\"Atlas {conf:.2f}\"\n",
    "            cv2.putText(\n",
    "                small, label, (x1, y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2\n",
    "            )\n",
    "\n",
    "        display_frame = cv2.resize(small, display_size)\n",
    "\n",
    "    cv2.imshow(\"Atlas Pose Estimation\", display_frame)\n",
    "\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    wait_ms = max(int((frame_interval - elapsed) * 1000), 1)\n",
    "    if cv2.waitKey(wait_ms) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1) Load your one‐class pose model\n",
    "pose_model = YOLO(\"best.pt\")\n",
    "\n",
    "# 2) Open the video\n",
    "cap = cv2.VideoCapture(\"Atlas_1.mp4\")\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open video\")\n",
    "\n",
    "# 3) Create a resizable window (we’ll handle scaling ourselves)\n",
    "window_name = \"Atlas Pose\"\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "# 4) Thresholds\n",
    "leg_dist_threshold = 50   # front paws closer than this => Sitting\n",
    "vertical_threshold = 30   # for deciding Laying vs Unknown\n",
    "\n",
    "# 5) Skeleton pairs (optional drawing)\n",
    "skeleton_pairs = [(0,5),(0,6),(5,11),(6,12),(11,15),(12,15)]\n",
    "\n",
    "# 6) Max display size to prevent OS clipping\n",
    "MAX_W, MAX_H = 800, 800\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 7) Copy for annotation\n",
    "    annotated = frame.copy()\n",
    "    h, w = annotated.shape[:2]\n",
    "\n",
    "    res = pose_model(frame)[0]\n",
    "    if res.keypoints is not None and len(res.keypoints.xy) > 0:\n",
    "        kp = res.keypoints.xy.numpy()[0]  # (N,2)\n",
    "\n",
    "        # compute front paws distance\n",
    "        if np.all(kp[[5,6],:] > 0):\n",
    "            leg_dist = np.linalg.norm(kp[5] - kp[6])\n",
    "        else:\n",
    "            leg_dist = None\n",
    "\n",
    "        front_y = np.mean(kp[[5,6],1]) if np.all(kp[[5,6],:] > 0) else None\n",
    "        back_y  = np.mean(kp[[11,12],1]) if np.all(kp[[11,12],:] > 0) else None\n",
    "\n",
    "        # poses logic\n",
    "        if leg_dist is not None and leg_dist < leg_dist_threshold:\n",
    "            pose = \"Sitting\"\n",
    "        elif front_y and back_y and abs(back_y - front_y) < vertical_threshold:\n",
    "            pose = \"Laying Down\"\n",
    "        else:\n",
    "            pose = \"Unknown\"\n",
    "\n",
    "        x1, y1 = kp.min(axis=0).astype(int)\n",
    "        x2, y2 = kp.max(axis=0).astype(int)\n",
    "        cv2.rectangle(annotated, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "\n",
    "        # Since top of video is cut off, adjust the label position\n",
    "        label_y = min(y2 + 25, h - 10)\n",
    "        cv2.putText(\n",
    "            annotated, pose,\n",
    "            (x1, label_y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1.2,\n",
    "            (0,255,0), 2\n",
    "        )\n",
    "\n",
    "        for (x,y) in kp:\n",
    "            if x>0 and y>0:\n",
    "                cv2.circle(annotated, (int(x),int(y)), 4, (0,0,255), -1)\n",
    "        for a,b in skeleton_pairs:\n",
    "            p1, p2 = kp[a], kp[b]\n",
    "            if np.all(p1>0) and np.all(p2>0):\n",
    "                cv2.line(\n",
    "                    annotated,\n",
    "                    tuple(p1.astype(int)),\n",
    "                    tuple(p2.astype(int)),\n",
    "                    (255,0,0), 2\n",
    "                )\n",
    "\n",
    "\n",
    "    scale = min(MAX_W / w, MAX_H / h, 1.0)\n",
    "    disp = cv2.resize(annotated, (int(w * scale), int(h * scale)))\n",
    "\n",
    "\n",
    "    cv2.imshow(window_name, disp)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
